accum_count:
- 1
accum_steps:
- 0
adagrad_accumulator_init: 0
adam_beta1: 0.9
adam_beta2: 0.999
alignment_heads: 0
alignment_layer: -3
apex_opt_level: O1
attention_dropout:
- 0.1
average_decay: 0
average_every: 1
batch_size: '32'
batch_type: sents
bidir_edges: true
bridge_extra_node: true
bucket_size: 2048
cnn_kernel_width: 3
config: .\EN_TH_data\en_th.yaml
data: '{''corpus_1'': {''path_src'': ''EN_TH_data/src_train.txt'', ''path_tgt'': ''EN_TH_data/tgt_train.txt''},
  ''valid'': {''path_src'': ''EN_TH_data/src_val.txt'', ''path_tgt'': ''EN_TH_data/tgt_val.txt''}}'
data_type: text
dec_layers: 2
dec_rnn_size: 500
decay_method: none
decay_steps: 10000
decoder_type: rnn
dropout:
- 0.3
dropout_steps:
- 0
early_stopping: 0
enc_layers: 2
enc_rnn_size: 500
encoder_type: rnn
epochs: 0
exp: ''
exp_host: ''
feat_merge: concat
feat_vec_exponent: 0.7
feat_vec_size: -1
generator_function: softmax
global_attention: general
global_attention_function: softmax
gpu_backend: nccl
gpu_ranks:
- 0
gpu_verbose_level: 0
gpuid: []
heads: 8
input_feed: 1
insert_ratio: 0.0
keep_checkpoint: -1
label_smoothing: 0.0
lambda_align: 0.0
lambda_coverage: 0.0
layers: -1
learning_rate: 1.0
learning_rate_decay: 0.5
log_file: ''
log_file_level: '0'
loss_scale: 0
mask_length: subword
mask_ratio: 0.0
master_ip: localhost
master_port: 10000
max_generator_batches: 32
max_grad_norm: 5
max_relative_positions: 0
model_dtype: fp32
model_task: seq2seq
model_type: text
n_edge_types: 2
n_node: 2
n_sample: 0
n_steps: 2
normalization: sents
optim: sgd
overwrite: 'False'
param_init: 0.1
permute_sent_ratio: 0.0
poisson_lambda: 3.0
pool_factor: 8192
pos_ffn_activation_fn: relu
queue_size: 40
random_ratio: 0.0
replace_length: -1
report_every: 50
reset_optim: none
reversible_tokenization: joiner
rnn_size: -1
rnn_type: LSTM
rotate_ratio: 0.0
save_checkpoint_steps: '10000'
save_config: EN_TH_data/saved_config_1.yaml
save_data: EN_TH_data/run/example
save_model: EN_TH_data/run/model
seed: -1
self_attn_type: scaled-dot
skip_empty_level: warning
src_ggnn_size: 0
src_onmttok_kwargs: '{''mode'': ''none''}'
src_seq_length: 200
src_subword_alpha: 0
src_subword_nbest: 1
src_subword_type: none
src_subword_vocab: ''
src_vocab: EN_TH_data/run/example.vocab.src
src_vocab_size: '70000'
src_vocab_threshold: 0
src_word_vec_size: 500
src_words_min_frequency: 0
start_decay_steps: 50000
state_dim: 512
switchout_temperature: 1.0
tensorboard_log_dir: runs/onmt
tgt_onmttok_kwargs: '{''mode'': ''none''}'
tgt_seq_length: 200
tgt_subword_alpha: 0
tgt_subword_nbest: 1
tgt_subword_type: none
tgt_subword_vocab: ''
tgt_vocab: EN_TH_data/run/example.vocab.tgt
tgt_vocab_size: '70000'
tgt_vocab_threshold: 0
tgt_word_vec_size: 500
tgt_words_min_frequency: 0
tokendrop_temperature: 1.0
tokenmask_temperature: 1.0
train_from: ''
train_steps: '1000000'
transformer_ff: 2048
transforms: []
truncated_decoder: 0
valid_batch_size: '16'
valid_steps: 10000
vocab_size_multiple: 1
warmup_steps: 4000
word_vec_size: -1
world_size: '1'
